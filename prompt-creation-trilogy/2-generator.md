# Adaptive Prompt Generation Loop Protocol
Ôªø
## INITIALIZATION
Enter **Adaptive Prompt Generation Mode**. Operate as an iterative prompt creation system using the OODA Loop (Observe, Orient, Decide, Act) to build a high-quality prompt from requirements specification.
Ôªø
### Loop Control Variables:
```
LOOP_NUMBER=0
COMPONENTS_ADDED=[]
PROMPT_VERSION="v0_requirements"
BUILD_LEVEL="foundation"
GENERATION_START_TIME=$(current timestamp)
QUALITY_SCORE=0
TARGET_SCORE=90
PROMPT_PURPOSE=""
```
Ôªø
## MANDATORY 6-LOOP COMPLETION
**CRITICAL RULE:** Always complete all 6 loops regardless of quality score achieved.
Ôªø
**Why 6 loops are always required:**
- Loop 1-2: Foundation layers
- Loop 3-4: Enhancement and capability layers
- Loop 5-6: Optimization and excellence layers
Ôªø
**No early termination allowed** - Each loop serves a distinct architectural purpose.
Ôªø
### Initialize Generation Log:
Create prompt generation log using Write tool:
- prompt_gen.md with session timestamp
- Requirements specification section
- Version tracking section
Ôªø
## PRE-LOOP REQUIREMENTS GATHERING
Capture the user's needs and establish generation targets:
- [ ] Document the desired prompt's purpose and goals
- [ ] Identify target AI model (GPT-4, Claude, etc.)
- [ ] Capture expected inputs and outputs
- [ ] Define success criteria and use cases
- [ ] Note any specific constraints or requirements
Ôªø
### Requirements Analysis Template:
```
## Requirements Analysis
Ôªø
# Core Requirements
PROMPT_PURPOSE="[What should this prompt accomplish?]"
TARGET_MODEL="[Which AI model will use this?]"
INPUT_TYPE="[What inputs will it process?]"
OUTPUT_TYPE="[What outputs should it produce?]"
USE_CASES=["Primary use case", "Secondary use case"]
Ôªø
# Functional Requirements
MUST_HAVE=["Critical feature 1", "Critical feature 2"]
SHOULD_HAVE=["Important feature 1", "Important feature 2"]
NICE_TO_HAVE=["Optional feature 1", "Optional feature 2"]
Ôªø
# Non-Functional Requirements
TONE_STYLE="[Professional/Casual/Technical/Creative]"
COMPLEXITY_LEVEL="[Simple/Moderate/Advanced/Expert]"
ERROR_HANDLING="[Required/Optional/None]"
PERFORMANCE_NEEDS="[Speed/Accuracy/Creativity balance]"
Ôªø
# Constraints
MAX_LENGTH="[Token/character limit if any]"
FORBIDDEN_ELEMENTS=["Things to avoid"]
REQUIRED_TECHNIQUES=["Specific techniques to use"]
```
Ôªø
---
Ôªø
## THE PROMPT GENERATION OODA LOOP
Ôªø
### ‚≠ï PHASE 0: ARCHITECTURE PLANNING
**Design the prompt structure based on requirements**
Ôªø
#### Dynamic Architecture Detection:
Ôªø
```
def detect_prompt_architecture(requirements):
    """
    Simple architecture detection based on basic requirements analysis
    """
    purpose = requirements.get('PROMPT_PURPOSE', '').lower()
    complexity = requirements.get('COMPLEXITY_LEVEL', 'moderate')
Ôªø
    # Determine required components based on simple heuristics
    components = ['main_instruction']  # Always needed
Ôªø
    # Add components based on purpose
    if any(word in purpose for word in ['analyze', 'analysis', 'examine']):
        components.extend(['reasoning_steps', 'validation_checks'])
    elif any(word in purpose for word in ['create', 'generate', 'build']):
        components.extend(['examples', 'validation_checks'])
    elif any(word in purpose for word in ['solve', 'problem', 'fix']):
        components.extend(['reasoning_steps', 'error_handling'])
Ôªø
    # Add based on complexity
    if complexity == 'advanced':
        if 'examples' not in components:
            components.append('examples')
        if 'error_handling' not in components:
            components.append('error_handling')
Ôªø
    # Determine build sequence
    build_sequence = {
        'foundation': ['main_instruction'],
        'enhancement': [comp for comp in components if comp != 'main_instruction'],
        'optimization': []
    }
Ôªø
    return {
        'architecture_type': 'adaptive_simple',
        'components': components,
        'build_sequence': build_sequence,
        'optimization_targets': ['clarity_and_effectiveness']
    }
```
Ôªø
### Log Loop Start:
Use Edit tool to update prompt_gen.md with:
```
## Loop {LOOP_NUMBER} - {timestamp}
**Build Stage:** {CURRENT_BUILD_STAGE}
**Goal:** Build {stage} layer
**Architecture:** {PROMPT_ARCHITECTURE}
```
Ôªø
---
Ôªø
### üîç PHASE 1: OBSERVE
**Analyze current state and identify what to build next**
Ôªø
#### Current State Assessment:
Update prompt_gen.md using Edit tool:
Ôªø
```
### OBSERVE - Current State
Ôªø
If LOOP_NUMBER == 1:
    **Starting Point:** Requirements only
    **Components Needed:** {REQUIRED_COMPONENTS}
Else:
    **Current Version:** {PROMPT_VERSION}
    **Components Built:** {COMPONENTS_ADDED}
    **Current Quality:** {QUALITY_SCORE}/100
Ôªø
    # Calculate missing components
    MISSING_COMPONENTS = [component for component in REQUIRED_COMPONENTS if component not in COMPONENTS_ADDED]
    **Missing Components:** {MISSING_COMPONENTS}
```
Ôªø
**Quality Gap Analysis Function:**
```
def analyze_quality_gaps(quality_score):
    gaps = []
    if quality_score < 30: gaps.append("Missing core instructions")
    if quality_score < 50: gaps.append("Needs clarity and structure") 
    if quality_score < 70: gaps.append("Requires error handling")
    if quality_score < 85: gaps.append("Could benefit from advanced techniques")
    if quality_score < 95: gaps.append("Fine-tuning needed")
    return gaps
```
Ôªø
---
Ôªø
### üß≠ PHASE 2: ORIENT
**Determine building strategy for this iteration**
Ôªø
#### Build Strategy Selection:
Ôªø
Update prompt_gen.md using Edit tool:
```
### ORIENT - Build Strategy
Ôªø
Build focus selection based on CURRENT_BUILD_STAGE:
Ôªø
foundation:
    BUILD_FOCUS = "Core Structure"
    TECHNIQUES_TO_ADD = []
    COMPONENTS_TO_BUILD = ["main_instruction", "basic_constraints", "output_format"]
    **Strategy:** Establish basic prompt skeleton
Ôªø
enhancement:
    BUILD_FOCUS = "Capability Enhancement"
    TECHNIQUES_TO_ADD = ["chain_of_thought", "examples"]
    COMPONENTS_TO_BUILD = ["reasoning_steps", "validation_checks"]
    **Strategy:** Add reasoning and validation layers
Ôªø
reasoning:
    BUILD_FOCUS = "Advanced Reasoning"
    TECHNIQUES_TO_ADD = ["self_consistency", "verification_loops"]
    COMPONENTS_TO_BUILD = ["thinking_framework", "decision_trees"]
    **Strategy:** Implement advanced reasoning chains
Ôªø
technical:
    BUILD_FOCUS = "Technical Integration"
    TECHNIQUES_TO_ADD = ["function_calls", "tool_usage"]
    COMPONENTS_TO_BUILD = ["api_integration", "error_recovery"]
    **Strategy:** Add technical capabilities
Ôªø
polish:
    BUILD_FOCUS = "Optimization & Refinement"
    TECHNIQUES_TO_ADD = ["edge_cases", "performance_tuning"]
    COMPONENTS_TO_BUILD = ["final_constraints", "quality_assurance"]
    **Strategy:** Final optimization and edge case handling
Ôªø
refinement:
    BUILD_FOCUS = "Iterative Improvement"
    TECHNIQUES_TO_ADD = ["identified_gaps"]
    COMPONENTS_TO_BUILD = ["specific_improvements"]
    **Strategy:** Address specific quality gaps
Ôªø
**Build Focus:** {BUILD_FOCUS}
**Components to Add:** {COMPONENTS_TO_BUILD}
**Techniques to Integrate:** {TECHNIQUES_TO_ADD}
```
Ôªø
#### Component Generation Templates:
Ôªø
```
def generate_component(component_type, requirements, current_prompt=""):
    """
    Simple but effective component generation based on requirements
    """
    purpose = requirements.get('PROMPT_PURPOSE', '')
    input_type = requirements.get('INPUT_TYPE', 'input')
    output_type = requirements.get('OUTPUT_TYPE', 'output')
    complexity = requirements.get('COMPLEXITY_LEVEL', 'moderate')
Ôªø
    if component_type == "main_instruction":
        return generate_main_instruction(purpose, input_type, output_type, complexity)
    elif component_type == "reasoning_steps":
        return generate_reasoning_steps(purpose, complexity)
    elif component_type == "error_handling":
        return generate_error_handling(purpose)
    elif component_type == "validation_checks":
        return generate_validation_checks(output_type, purpose)
    elif component_type == "examples":
        return generate_examples(purpose, input_type, output_type)
    else:
        return f"## {component_type.replace('_', ' ').title()}\n[Component for {purpose}]"
Ôªø
def generate_main_instruction(purpose, input_type, output_type, complexity):
    """Generate clear main instruction based on simple parameters"""
Ôªø
    if 'analyze' in purpose.lower():
        instruction = f"Analyze the provided {input_type} and provide {output_type}."
        if complexity == 'advanced':
            instruction += " Use systematic analytical methods and provide detailed insights."
Ôªø
    elif 'create' in purpose.lower() or 'generate' in purpose.lower():
        instruction = f"Create {output_type} based on the {input_type}."
        if complexity == 'advanced':
            instruction += " Follow best practices and ensure high-quality output."
Ôªø
    elif 'solve' in purpose.lower() or 'problem' in purpose.lower():
        instruction = f"Solve the problem presented in the {input_type} and provide {output_type}."
        if complexity == 'advanced':
            instruction += " Use step-by-step problem-solving methodology."
    else:
        instruction = f"Process the {input_type} to produce {output_type}."
Ôªø
    return instruction
Ôªø
def generate_reasoning_steps(purpose, complexity):
    """Generate context-specific reasoning steps based on purpose analysis"""
Ôªø
    purpose_lower = purpose.lower()
Ôªø
    # Analyze what kind of analysis this is
    if any(word in purpose_lower for word in ['sentiment', 'emotion', 'feeling', 'opinion']):
        return generate_sentiment_analysis_steps(complexity)
    elif any(word in purpose_lower for word in ['data', 'statistics', 'metrics', 'numbers']):
        return generate_data_analysis_steps(complexity)
    elif any(word in purpose_lower for word in ['text', 'document', 'content', 'writing']):
        return generate_text_analysis_steps(complexity)
    elif any(word in purpose_lower for word in ['market', 'business', 'financial', 'economic']):
        return generate_business_analysis_steps(complexity)
    elif any(word in purpose_lower for word in ['code', 'software', 'technical', 'system']):
        return generate_technical_analysis_steps(complexity)
    elif 'analyze' in purpose_lower:
        return generate_generic_analysis_steps(complexity)
    elif any(word in purpose_lower for word in ['problem', 'solve', 'issue', 'troubleshoot']):
        return generate_problem_solving_steps(purpose_lower, complexity)
    elif any(word in purpose_lower for word in ['create', 'generate', 'build', 'design']):
        return generate_creation_steps(purpose_lower, complexity)
    else:
        return generate_generic_systematic_steps(complexity)
Ôªø
def generate_sentiment_analysis_steps(complexity):
    """Steps specifically for sentiment/emotion analysis"""
    if complexity == 'simple':
        return """Sentiment Analysis Approach:
1. Identify emotional indicators (positive/negative words)
2. Consider context and tone
3. Determine overall sentiment"""
    else:
        return """Comprehensive Sentiment Analysis:
1. Extract explicit emotional language and sentiment indicators
2. Analyze implicit tone, context, and subtext
3. Consider cultural and domain-specific nuances
4. Evaluate sentiment intensity and confidence levels
5. Provide balanced assessment with supporting evidence"""
Ôªø
def generate_data_analysis_steps(complexity):
    """Steps specifically for data/statistical analysis"""
    if complexity == 'simple':
        return """Data Analysis Approach:
1. Examine the data structure and key metrics
2. Identify trends, patterns, and outliers
3. Draw insights from the findings"""
    else:
        return """Systematic Data Analysis:
1. Assess data quality, completeness, and reliability
2. Apply appropriate statistical methods and frameworks
3. Identify significant patterns, correlations, and anomalies
4. Consider potential confounding factors and limitations
5. Generate actionable insights with confidence intervals"""
Ôªø
def generate_text_analysis_steps(complexity):
    """Steps specifically for text/content analysis"""
    if complexity == 'simple':
        return """Text Analysis Approach:
1. Identify main themes and key concepts
2. Examine structure, style, and tone
3. Summarize findings and implications"""
    else:
        return """Comprehensive Text Analysis:
1. Conduct structural analysis (organization, flow, coherence)
2. Perform thematic analysis (main ideas, supporting arguments)
3. Evaluate linguistic features (style, tone, rhetoric)
4. Assess effectiveness for intended purpose and audience
5. Identify strengths, weaknesses, and recommendations"""
Ôªø
def generate_business_analysis_steps(complexity):
    """Steps specifically for business/market analysis"""
    if complexity == 'simple':
        return """Business Analysis Approach:
1. Identify key business factors and stakeholders
2. Assess opportunities and risks
3. Provide strategic recommendations"""
    else:
        return """Strategic Business Analysis:
1. Analyze market context and competitive landscape
2. Evaluate internal capabilities and external factors
3. Assess financial implications and resource requirements
4. Identify risks, opportunities, and strategic options
5. Develop actionable recommendations with implementation considerations"""
Ôªø
def generate_technical_analysis_steps(complexity):
    """Steps specifically for technical/code analysis"""
    if complexity == 'simple':
        return """Technical Analysis Approach:
1. Review architecture and implementation approach
2. Identify potential issues and improvements
3. Provide technical recommendations"""
    else:
        return """Comprehensive Technical Analysis:
1. Evaluate system architecture and design patterns
2. Assess code quality, performance, and maintainability
3. Identify security vulnerabilities and technical debt
4. Consider scalability and future requirements
5. Recommend specific improvements with priority levels"""
Ôªø
def generate_problem_solving_steps(purpose_lower, complexity):
    """Context-specific problem solving based on problem type"""
    if any(word in purpose_lower for word in ['debug', 'error', 'bug', 'fix']):
        return """Debugging Approach:
1. Reproduce and isolate the problem
2. Analyze symptoms and potential root causes
3. Test solutions systematically
4. Verify the fix and prevent recurrence"""
    elif any(word in purpose_lower for word in ['optimize', 'improve', 'enhance']):
        return """Optimization Approach:
1. Establish current baseline and success metrics
2. Identify bottlenecks and improvement opportunities  
3. Design and test potential optimizations
4. Implement changes and measure results"""
    else:
        return """Problem-Solving Framework:
1. Define the problem clearly and completely
2. Generate multiple potential solutions
3. Evaluate options against key criteria
4. Implement the best solution and verify results"""
Ôªø
def generate_creation_steps(purpose_lower, complexity):
    """Context-specific creation steps based on what's being created"""
    if any(word in purpose_lower for word in ['strategy', 'plan', 'framework']):
        return """Strategic Creation Process:
1. Define objectives and success criteria
2. Research best practices and gather requirements
3. Design comprehensive framework or strategy
4. Validate approach and refine based on feedback"""
    elif any(word in purpose_lower for word in ['content', 'write', 'document']):
        return """Content Creation Process:
1. Define target audience and key messages
2. Create structured outline with main points
3. Develop content with appropriate tone and style
4. Review for clarity, accuracy, and effectiveness"""
    else:
        return """Creation Framework:
1. Define requirements and constraints
2. Research and gather relevant inputs
3. Design and build systematically
4. Test, refine, and finalize output"""
Ôªø
def generate_generic_analysis_steps(complexity):
    """Fallback for generic analysis tasks"""
    if complexity == 'simple':
        return """Analysis Approach:
1. Examine key components and relationships
2. Identify significant patterns or findings
3. Draw evidence-based conclusions"""
    else:
        return """Systematic Analysis Framework:
1. Define scope, objectives, and analytical approach
2. Gather and organize relevant information systematically
3. Apply appropriate analytical methods and frameworks
4. Synthesize findings and identify key insights
5. Present conclusions with supporting evidence and limitations"""
Ôªø
def generate_generic_systematic_steps(complexity):
    """Most generic fallback for any systematic task"""
    return """Systematic Approach:
1. Understand requirements and context
2. Plan your methodology and approach
3. Execute systematically with attention to detail
4. Review results and refine as needed"""
Ôªø
def generate_error_handling(purpose):
    """Generate simple error handling guidance"""
    return """If you encounter issues:
- State what clarification is needed
- Make reasonable assumptions if necessary
- Flag any limitations in your response"""
Ôªø
def generate_validation_checks(output_type, purpose):
    """Generate simple validation checks"""
    checks = ["Before finalizing, verify:"]
Ôªø
    if 'analysis' in purpose.lower():
        checks.extend([
            "- Key points are addressed",
            "- Conclusions are supported",
            "- Analysis is complete"
        ])
    elif 'creative' in purpose.lower():
        checks.extend([
            "- Output meets requirements",
            "- Creative elements are appropriate",
            "- Quality is high"
        ])
    else:
        checks.extend([
            "- Requirements are met",
            "- Quality is adequate",
            "- Output is complete"
        ])
Ôªø
    return '\n'.join(checks)
Ôªø
def generate_examples(purpose, input_type, output_type):
    """Generate context-specific examples based on purpose"""
    purpose_lower = purpose.lower()
Ôªø
    # Generate relevant examples based on task type
    if any(word in purpose_lower for word in ['sentiment', 'emotion', 'opinion']):
        return generate_sentiment_examples(input_type, output_type)
    elif any(word in purpose_lower for word in ['data', 'statistics', 'metrics']):
        return generate_data_examples(input_type, output_type)
    elif any(word in purpose_lower for word in ['code', 'technical', 'software']):
        return generate_technical_examples(input_type, output_type)
    elif any(word in purpose_lower for word in ['business', 'market', 'strategy']):
        return generate_business_examples(input_type, output_type)
    elif any(word in purpose_lower for word in ['text', 'document', 'content']):
        return generate_text_examples(input_type, output_type)
    elif 'analyze' in purpose_lower:
        return generate_analysis_examples(input_type, output_type, purpose)
    else:
        return generate_generic_examples(input_type, output_type, purpose)
Ôªø
def generate_sentiment_examples(input_type, output_type):
    return f"""## Example
Ôªø
**Input:** "I absolutely love this new feature! It makes everything so much easier and saves me tons of time."
Ôªø
**Output:** 
- **Sentiment:** Positive (Strong)
- **Confidence:** High (95%)
- **Key Indicators:** "absolutely love", "so much easier", "saves me tons of time"
- **Emotional Tone:** Enthusiastic and appreciative
- **Recommendation:** Feature is well-received, consider highlighting in marketing"""
Ôªø
def generate_data_examples(input_type, output_type):
    return f"""## Example
Ôªø
**Input:** Sales data showing Q4: $2.3M, Q3: $1.8M, Q2: $1.9M, Q1: $1.6M
Ôªø
**Output:**
- **Trend:** Consistent growth with 28% increase Q3‚ÜíQ4
- **Key Pattern:** Strong Q4 performance (+44% vs Q1)
- **Notable:** Q2 slight dip, but recovery trend established
- **Projection:** If trend continues, Q1 next year could reach $2.8M
- **Recommendation:** Investigate Q4 success factors for replication"""
Ôªø
def generate_technical_examples(input_type, output_type):
    return f"""## Example
Ôªø
**Input:** Code snippet with nested loops processing large arrays
Ôªø
**Output:**
- **Performance Issue:** O(n¬≤) time complexity in nested loops
- **Memory Concern:** Unnecessary array copies in inner loop
- **Maintainability:** Deep nesting reduces readability
- **Security:** No input validation on array bounds
- **Recommendation:** Implement single-pass algorithm with HashMap, add input validation"""
Ôªø
def generate_business_examples(input_type, output_type):
    return f"""## Example
Ôªø
**Input:** Market analysis request for entering the electric vehicle charging market
Ôªø
**Output:**
- **Market Size:** $31B globally, growing 25% annually
- **Key Players:** ChargePoint, Tesla Supercharger, EVgo
- **Barriers:** High infrastructure costs, regulatory complexity
- **Opportunities:** Partnership with retail chains, government incentives
- **Risk Assessment:** Medium-high initial investment, long-term growth potential
- **Recommendation:** Start with strategic partnerships before direct investment"""
Ôªø
def generate_text_examples(input_type, output_type):
    return f"""## Example
Ôªø
**Input:** Marketing email draft for product launch
Ôªø
**Output:**
- **Clarity:** Subject line clear but could be more compelling
- **Structure:** Good flow from problem‚Üísolution‚Üíaction
- **Tone:** Professional but lacks excitement for launch
- **Call-to-Action:** Clear but positioned too late in email
- **Improvements:** Add urgency, move CTA higher, include social proof
- **Overall Grade:** B+ (effective but could be optimized)"""
Ôªø
def generate_analysis_examples(input_type, output_type, purpose):
    return f"""## Example
Ôªø
**Input:** {input_type} requiring {purpose}
Ôªø
**Output:**
- **Key Findings:** [Main discoveries from analysis]
- **Supporting Evidence:** [Data/facts that support conclusions]
- **Patterns Identified:** [Significant trends or relationships]
- **Implications:** [What these findings mean]
- **Recommendations:** [Actionable next steps based on analysis]
- **Confidence Level:** [How certain you are of conclusions]"""
Ôªø
def generate_generic_examples(input_type, output_type, purpose):
    return f"""## Example
Ôªø
**Input:** [Realistic sample {input_type}]
Ôªø
**Output:** [Detailed example of {output_type} that demonstrates:]
- Quality and depth expected
- Appropriate format and structure  
- Level of analysis or detail required
- Tone and style for {purpose}
Ôªø
**Note:** This example shows the standard expected for this type of {purpose} task."""
```
Ôªø
---
Ôªø
### üéØ PHASE 3: DECIDE
**Select specific components to build this iteration**
Ôªø
Update prompt_gen.md using Edit tool:
```
### DECIDE - Implementation Plan
Ôªø
# Prioritize what to build
PRIORITY_COMPONENT = COMPONENTS_TO_BUILD[0]
IMPLEMENTATION_METHOD = "[How to implement this component]"
INTEGRATION_POINT = "[Where in prompt structure]"
EXPECTED_IMPACT = "[Quality improvement expected]"
Ôªø
**Primary Component:** {PRIORITY_COMPONENT}
**Implementation Method:** {IMPLEMENTATION_METHOD}
**Integration Point:** {INTEGRATION_POINT}
**Expected Quality Gain:** +10-15 points
Ôªø
# Design the component
COMPONENT_TEXT = generate_component(PRIORITY_COMPONENT)
**Component Design:**
```
{COMPONENT_TEXT}
```
```
Ôªø
---
Ôªø
### ‚ö° PHASE 4: ACT
**Build and integrate the new component**
Ôªø
Update prompt_gen.md using Edit tool:
```
### ACT - Building Prompt
Ôªø
If LOOP_NUMBER == 1:
    # Create initial prompt structure using Write tool
    CURRENT_PROMPT = f"""# {PROMPT_PURPOSE}
Ôªø
## Instructions
{COMPONENT_TEXT}
Ôªø
## Input
[Specify input format]
Ôªø
## Output
[Specify output format]"""
Ôªø
Else:
    # Add to existing prompt using Edit tool
    CURRENT_PROMPT = f"""{PREVIOUS_PROMPT}
Ôªø
## {PRIORITY_COMPONENT}
{COMPONENT_TEXT}"""
Ôªø
# Update version and tracking
PROMPT_VERSION = f"v{LOOP_NUMBER}_{CURRENT_BUILD_STAGE}"
COMPONENTS_ADDED.append(PRIORITY_COMPONENT)
Ôªø
**Version Created:** {PROMPT_VERSION}
**Components Integrated:** {len(COMPONENTS_ADDED)} total
Ôªø
**Current Prompt:**
```markdown
{CURRENT_PROMPT}
```
Ôªø
# Store for next iteration
PREVIOUS_PROMPT = CURRENT_PROMPT
```
Ôªø
---
Ôªø
### üîÑ PHASE 5: CHECK & RE-LOOP
**Evaluate the generated prompt and determine next action**
Ôªø
Update prompt_gen.md using Edit tool:
```
### CHECK - Quality Assessment
Ôªø
def evaluate_prompt(current_prompt, requirements):
    """
    Realistic prompt quality evaluation with simple but meaningful assessment
    """
    assessment = {
        'clarity_score': 0,
        'completeness_score': 0, 
        'effectiveness_score': 0,
        'robustness_score': 0,
        'issues': [],
        'strengths': []
    }
Ôªø
    # CLARITY ASSESSMENT (0-25 points) - Simple but meaningful
    if len(current_prompt.strip()) > 50:
        assessment['clarity_score'] += 5
        assessment['strengths'].append("Substantial instruction content")
    else:
        assessment['issues'].append("Too brief - needs more detailed instructions")
Ôªø
    if any(word in current_prompt.lower() for word in ['task', 'goal', 'objective']):
        assessment['clarity_score'] += 10
        assessment['strengths'].append("Clear task definition present")
    else:
        assessment['issues'].append("Missing clear task definition")
Ôªø
    # Check for ambiguous language
    ambiguous_terms = ['might', 'maybe', 'perhaps', 'things', 'stuff', 'appropriate', 'good']
    ambiguity_count = sum(1 for term in ambiguous_terms if term in current_prompt.lower())
    if ambiguity_count == 0:
        assessment['clarity_score'] += 10
        assessment['strengths'].append("Language is precise and specific")
    elif ambiguity_count <= 2:
        assessment['clarity_score'] += 5
    else:
        assessment['issues'].append(f"Contains {ambiguity_count} ambiguous terms")
Ôªø
    # COMPLETENESS ASSESSMENT (0-25 points)
    essential_sections = ['input', 'output', 'instruction']
    present_sections = [section for section in essential_sections 
                       if section in current_prompt.lower()]
Ôªø
    assessment['completeness_score'] = len(present_sections) * 8
    if len(present_sections) == 3:
        assessment['strengths'].append("All essential sections present")
    else:
        missing = [s for s in essential_sections if s not in present_sections]
        assessment['issues'].append(f"Missing sections: {missing}")
Ôªø
    # EFFECTIVENESS ASSESSMENT (0-25 points) - Context-specific evaluation
    purpose = requirements['PROMPT_PURPOSE'].lower()
Ôªø
    # Domain-specific effectiveness checks
    if any(word in purpose for word in ['sentiment', 'emotion', 'opinion']):
        effectiveness_score = assess_sentiment_analysis_effectiveness(current_prompt)
        assessment['effectiveness_score'] += effectiveness_score['score']
        assessment['strengths'].extend(effectiveness_score['strengths'])
        assessment['issues'].extend(effectiveness_score['issues'])
Ôªø
    elif any(word in purpose for word in ['data', 'statistics', 'metrics']):
        effectiveness_score = assess_data_analysis_effectiveness(current_prompt)
        assessment['effectiveness_score'] += effectiveness_score['score']
        assessment['strengths'].extend(effectiveness_score['strengths'])
        assessment['issues'].extend(effectiveness_score['issues'])
Ôªø
    elif any(word in purpose for word in ['code', 'technical', 'software']):
        effectiveness_score = assess_technical_effectiveness(current_prompt)
        assessment['effectiveness_score'] += effectiveness_score['score']
        assessment['strengths'].extend(effectiveness_score['strengths'])
        assessment['issues'].extend(effectiveness_score['issues'])
Ôªø
    elif any(word in purpose for word in ['business', 'market', 'strategy']):
        effectiveness_score = assess_business_effectiveness(current_prompt)
        assessment['effectiveness_score'] += effectiveness_score['score']
        assessment['strengths'].extend(effectiveness_score['strengths'])
        assessment['issues'].extend(effectiveness_score['issues'])
Ôªø
    elif 'analyze' in purpose or 'analysis' in purpose:
        effectiveness_score = assess_analysis_effectiveness(current_prompt, purpose)
        assessment['effectiveness_score'] += effectiveness_score['score']
        assessment['strengths'].extend(effectiveness_score['strengths'])
        assessment['issues'].extend(effectiveness_score['issues'])
Ôªø
    elif 'create' in purpose or 'generate' in purpose:
        effectiveness_score = assess_creation_effectiveness(current_prompt, purpose)
        assessment['effectiveness_score'] += effectiveness_score['score']
        assessment['strengths'].extend(effectiveness_score['strengths'])
        assessment['issues'].extend(effectiveness_score['issues'])
Ôªø
    elif 'solve' in purpose or 'problem' in purpose:
        effectiveness_score = assess_problem_solving_effectiveness(current_prompt, purpose)
        assessment['effectiveness_score'] += effectiveness_score['score']
        assessment['strengths'].extend(effectiveness_score['strengths'])
        assessment['issues'].extend(effectiveness_score['issues'])
Ôªø
    else:
        # Generic effectiveness check with basic alignment
        alignment_score = assess_generic_effectiveness(current_prompt, requirements)
        assessment['effectiveness_score'] += alignment_score['score']
        assessment['strengths'].extend(alignment_score['strengths'])
        assessment['issues'].extend(alignment_score['issues'])
Ôªø
def assess_sentiment_analysis_effectiveness(prompt):
    """Assess effectiveness specifically for sentiment analysis tasks"""
    score = 0
    strengths = []
    issues = []
Ôªø
    if any(word in prompt.lower() for word in ['sentiment', 'emotion', 'tone']):
        score += 8
        strengths.append("Explicitly mentions sentiment/emotion analysis")
    else:
        issues.append("Should explicitly reference sentiment or emotional analysis")
Ôªø
    if any(word in prompt.lower() for word in ['context', 'nuance', 'subtext']):
        score += 7
        strengths.append("Considers contextual factors beyond surface language")
    else:
        issues.append("Should emphasize importance of context in sentiment analysis")
Ôªø
    if any(word in prompt.lower() for word in ['confidence', 'intensity', 'degree']):
        score += 5
        strengths.append("Addresses sentiment strength/confidence")
    else:
        issues.append("Should specify need to assess sentiment intensity")
Ôªø
    if any(word in prompt.lower() for word in ['bias', 'objective', 'balanced']):
        score += 5
        strengths.append("Includes objectivity considerations")
Ôªø
    return {'score': score, 'strengths': strengths, 'issues': issues}
Ôªø
def assess_data_analysis_effectiveness(prompt):
    """Assess effectiveness specifically for data analysis tasks"""
    score = 0
    strengths = []
    issues = []
Ôªø
    if any(word in prompt.lower() for word in ['pattern', 'trend', 'correlation']):
        score += 8
        strengths.append("Focuses on identifying data patterns and trends")
    else:
        issues.append("Should emphasize pattern and trend identification")
Ôªø
    if any(word in prompt.lower() for word in ['statistical', 'significance', 'confidence']):
        score += 7
        strengths.append("Considers statistical rigor and significance")
    else:
        issues.append("Should address statistical significance and confidence")
Ôªø
    if any(word in prompt.lower() for word in ['outlier', 'anomaly', 'exception']):
        score += 5
        strengths.append("Includes attention to outliers and anomalies")
Ôªø
    if any(word in prompt.lower() for word in ['limitation', 'assumption', 'caveat']):
        score += 5
        strengths.append("Acknowledges analytical limitations")
    else:
        issues.append("Should mention analytical limitations and assumptions")
Ôªø
    return {'score': score, 'strengths': strengths, 'issues': issues}
Ôªø
def assess_technical_effectiveness(prompt):
    """Assess effectiveness for technical/code analysis"""
    score = 0
    strengths = []
    issues = []
Ôªø
    if any(word in prompt.lower() for word in ['performance', 'scalability', 'efficiency']):
        score += 8
        strengths.append("Addresses performance and scalability concerns")
    else:
        issues.append("Should consider performance and scalability factors")
Ôªø
    if any(word in prompt.lower() for word in ['security', 'vulnerability', 'risk']):
        score += 7
        strengths.append("Includes security considerations")
    else:
        issues.append("Should address security implications")
Ôªø
    if any(word in prompt.lower() for word in ['maintainability', 'readable', 'clean']):
        score += 5
        strengths.append("Considers code quality and maintainability")
Ôªø
    if any(word in prompt.lower() for word in ['best practice', 'standard', 'convention']):
        score += 5
        strengths.append("References industry best practices")
    else:
        issues.append("Should reference relevant technical standards")
Ôªø
    return {'score': score, 'strengths': strengths, 'issues': issues}
Ôªø
def assess_business_effectiveness(prompt):
    """Assess effectiveness for business analysis"""
    score = 0
    strengths = []
    issues = []
Ôªø
    if any(word in prompt.lower() for word in ['stakeholder', 'impact', 'business value']):
        score += 8
        strengths.append("Considers stakeholder impact and business value")
    else:
        issues.append("Should address stakeholder impact and business value")
Ôªø
    if any(word in prompt.lower() for word in ['risk', 'opportunity', 'threat']):
        score += 7
        strengths.append("Includes risk and opportunity assessment")
    else:
        issues.append("Should evaluate risks and opportunities")
Ôªø
    if any(word in prompt.lower() for word in ['actionable', 'implement', 'execute']):
        score += 5
        strengths.append("Emphasizes actionable recommendations")
    else:
        issues.append("Should focus on actionable business recommendations")
Ôªø
    if any(word in prompt.lower() for word in ['roi', 'cost', 'benefit', 'resource']):
        score += 5
        strengths.append("Addresses resource and financial considerations")
Ôªø
    return {'score': score, 'strengths': strengths, 'issues': issues}
Ôªø
def assess_analysis_effectiveness(prompt, purpose):
    """Generic analysis effectiveness assessment"""
    score = 0
    strengths = []
    issues = []
Ôªø
    if any(word in prompt.lower() for word in ['systematic', 'methodical', 'structured']):
        score += 10
        strengths.append("Promotes systematic analytical approach")
    else:
        issues.append("Should emphasize systematic analytical methodology")
Ôªø
    if any(word in prompt.lower() for word in ['evidence', 'support', 'justify']):
        score += 8
        strengths.append("Requires evidence-based conclusions")
    else:
        issues.append("Should require evidence to support conclusions")
Ôªø
    if any(word in prompt.lower() for word in ['comprehensive', 'thorough', 'complete']):
        score += 7
        strengths.append("Emphasizes thoroughness in analysis")
Ôªø
    return {'score': score, 'strengths': strengths, 'issues': issues}
Ôªø
def assess_creation_effectiveness(prompt, purpose):
    """Assess effectiveness for creative/generative tasks"""
    score = 0
    strengths = []
    issues = []
Ôªø
    if 'example' in prompt.lower():
        score += 10
        strengths.append("Provides examples to guide creation")
    else:
        issues.append("Should include examples to guide creative output")
Ôªø
    if any(word in prompt.lower() for word in ['format', 'structure', 'template']):
        score += 8
        strengths.append("Specifies output format and structure")
    else:
        issues.append("Should specify desired format and structure")
Ôªø
    if any(word in prompt.lower() for word in ['quality', 'standard', 'criteria']):
        score += 7
        strengths.append("Establishes quality standards")
    else:
        issues.append("Should establish quality criteria for output")
Ôªø
    return {'score': score, 'strengths': strengths, 'issues': issues}
Ôªø
def assess_problem_solving_effectiveness(prompt, purpose):
    """Assess effectiveness for problem-solving tasks"""
    score = 0
    strengths = []
    issues = []
Ôªø
    if any(word in prompt.lower() for word in ['step', 'approach', 'method', 'process']):
        score += 12
        strengths.append("Provides structured problem-solving approach")
    else:
        issues.append("Should provide clear problem-solving methodology")
Ôªø
    if any(word in prompt.lower() for word in ['root cause', 'underlying', 'source']):
        score += 8
        strengths.append("Emphasizes identifying root causes")
    else:
        issues.append("Should emphasize finding root causes, not just symptoms")
Ôªø
    if any(word in prompt.lower() for word in ['solution', 'alternative', 'option']):
        score += 5
        strengths.append("Encourages exploring multiple solutions")
Ôªø
    return {'score': score, 'strengths': strengths, 'issues': issues}
Ôªø
def assess_generic_effectiveness(prompt, requirements):
    """Basic effectiveness assessment for unspecified tasks"""
    score = 0
    strengths = []
    issues = []
Ôªø
    # Check purpose alignment
    purpose_words = requirements['PROMPT_PURPOSE'].lower().split()
    alignment_count = sum(1 for word in purpose_words if word in prompt.lower())
Ôªø
    if alignment_count > len(purpose_words) * 0.5:
        score += 10
        strengths.append("Good alignment with stated purpose")
    else:
        score += 5
        issues.append("Could improve alignment with stated purpose")
Ôªø
    # Check for detail and structure
    if len(prompt) > 200:
        score += 8
        strengths.append("Provides detailed guidance")
    elif len(prompt) > 100:
        score += 5
    else:
        issues.append("Needs more detailed instructions")
Ôªø
    if any(word in prompt.lower() for word in ['step', 'process', 'approach']):
        score += 7
        strengths.append("Includes structured approach")
    else:
        issues.append("Should provide structured approach or methodology")
Ôªø
    return {'score': score, 'strengths': strengths, 'issues': issues}
Ôªø
# Continue with ROBUSTNESS ASSESSMENT (0-25 points) - Simplified
def evaluate_robustness(current_prompt):
    """Evaluate robustness of the prompt"""
    score = 0
    strengths = []
    issues = []
Ôªø
    if 'error' in current_prompt.lower() or 'if' in current_prompt.lower():
        score += 10
        strengths.append("Includes error handling guidance")
    else:
        issues.append("No error handling or edge case guidance")
Ôªø
    if 'verify' in current_prompt.lower() or 'check' in current_prompt.lower():
        score += 10
        strengths.append("Includes verification steps")
    else:
        issues.append("Missing verification/quality check steps")
Ôªø
    if len(current_prompt) > 300:  # Longer prompts tend to be more robust
        score += 5
        strengths.append("Comprehensive instruction set")
Ôªø
    return {'score': score, 'strengths': strengths, 'issues': issues}
Ôªø
# Complete the evaluate_prompt function
def complete_evaluate_prompt(current_prompt, requirements):
    """Complete evaluation function with all assessments"""
    assessment = {
        'clarity_score': 0,
        'completeness_score': 0, 
        'effectiveness_score': 0,
        'robustness_score': 0,
        'issues': [],
        'strengths': []
    }
Ôªø
    # Run all assessments and combine
    robustness_result = evaluate_robustness(current_prompt)
    assessment['robustness_score'] = robustness_result['score']
    assessment['strengths'].extend(robustness_result['strengths'])
    assessment['issues'].extend(robustness_result['issues'])
Ôªø
    total_score = (assessment['clarity_score'] + 
                  assessment['completeness_score'] +
                  assessment['effectiveness_score'] +
                  assessment['robustness_score'])
Ôªø
    return total_score, assessment
Ôªø
# Add utility functions
def format_list(items):
    """Format list items for display"""
    if not items:
        return "None identified"
    return '\n'.join(f"  - {item}" for item in items)
Ôªø
# Updated evaluation call
NEW_QUALITY_SCORE, DETAILED_ASSESSMENT = evaluate_prompt(CURRENT_PROMPT, requirements)
SCORE_IMPROVEMENT = NEW_QUALITY_SCORE - QUALITY_SCORE
QUALITY_SCORE = NEW_QUALITY_SCORE
Ôªø
**Previous Score:** {QUALITY_SCORE - SCORE_IMPROVEMENT}/100
**New Score:** {QUALITY_SCORE}/100
**Improvement:** +{SCORE_IMPROVEMENT} points
Ôªø
**Detailed Assessment:**
- Clarity: {DETAILED_ASSESSMENT['clarity_score']}/25
- Completeness: {DETAILED_ASSESSMENT['completeness_score']}/25  
- Effectiveness: {DETAILED_ASSESSMENT['effectiveness_score']}/25
- Robustness: {DETAILED_ASSESSMENT['robustness_score']}/25
Ôªø
**Identified Strengths:**
{format_list(DETAILED_ASSESSMENT['strengths'])}
Ôªø
**Issues to Address:**
{format_list(DETAILED_ASSESSMENT['issues'])}
Ôªø
**Purpose-Specific Effectiveness Analysis:**
{generate_purpose_effectiveness_analysis(CURRENT_PROMPT, requirements, DETAILED_ASSESSMENT)}
Ôªø
# Determine next action
if LOOP_NUMBER >= 6:
    NEXT_ACTION = "Complete" # All 6 loops completed
else:
    NEXT_ACTION = "Continue" # Continue to next required loop
Ôªø
### LOOP SUMMARY
**Quality Achievement:** {QUALITY_SCORE}/{TARGET_SCORE}
**Components Built:** {len(COMPONENTS_ADDED)}/{len(REQUIRED_COMPONENTS)}
**Next Action:** {NEXT_ACTION}
```
Ôªø
---
Ôªø
## QUALITY SCORE PURPOSE
Quality scores are for **tracking improvement**, not termination decisions.
- Scores help identify what each loop contributes
- High early scores may indicate missing advanced components
- Final score only meaningful after all 6 loops completed
- Target score is a guideline, not a stopping condition
Ôªø
### LOOP PROGRESSION LOGIC
- Loops 1-5: Always continue to next loop
- Loop 6: Complete with final assessment
Ôªø
**Current Loop Status:**
- If LOOP_NUMBER < 6: Prepare for Loop {LOOP_NUMBER + 1}
- If LOOP_NUMBER = 6: Finalize prompt generation
Ôªø
## LOOP COMPLETION VERIFICATION
Each loop must address its designated purpose:
- Loop 1: ‚úÖ Foundation established
- Loop 2: ‚úÖ Core methodology implemented
- Loop 3: ‚úÖ Examples and validation added
- Loop 4: ‚úÖ Advanced techniques integrated
- Loop 5: ‚úÖ Edge cases and optimization handled
- Loop 6: ‚úÖ Meta-instructions and final polish completed
Ôªø
**Only mark complete when all 6 purposes fulfilled.**
Ôªø
## üèÜ COMPLETION PROTOCOL
**Execute when all 6 loops have been completed**
Ôªø
### Final Generation Report:
Use Edit tool to add to prompt_gen.md:
```
## FINAL GENERATED PROMPT - {timestamp}
**Version:** {PROMPT_VERSION}
**Final Quality Score:** {QUALITY_SCORE}/100
**Build Loops:** {LOOP_NUMBER}
**Components Integrated:** {COMPONENTS_ADDED}
```
Ôªø
### Quality Certification:
```markdown
## Quality Assessment
Ôªø
| Aspect | Score | Status |
|--------|-------|--------|
| Completeness | [X/25] | ‚úÖ All required components present |
| Clarity | [X/25] | ‚úÖ Instructions are clear and unambiguous |
| Robustness | [X/25] | ‚úÖ Error handling and edge cases covered |
| Efficiency | [X/25] | ‚úÖ Optimized for token usage and performance |
Ôªø
**Overall Rating:** {QUALITY_SCORE}/100 - Production Ready
```
Ôªø
### üìÅ File Output Generation:
Use Write tool to create output file:
```
# Create output file with timestamp
TIMESTAMP = current_timestamp()
OUTPUT_FILE = f"prompt_clean_{TIMESTAMP}.md"
Ôªø
# Save clean prompt only using Write tool
Write(OUTPUT_FILE, CURRENT_PROMPT)
Ôªø
# Update prompt_gen.md with completion info
**Output File:** {OUTPUT_FILE}
**Quality Score:** {QUALITY_SCORE}/100
**Build Loops:** {LOOP_NUMBER}
Ôªø
# Display completion message
‚úÖ Prompt generation complete!
üìä Quality Score: {QUALITY_SCORE}/100
üèóÔ∏è Components Built: {len(COMPONENTS_ADDED)}
üîÑ Build Loops: {LOOP_NUMBER}
üìÅ Generated prompt saved to: {OUTPUT_FILE}
```
Ôªø
def generate_purpose_effectiveness_analysis(prompt, requirements, assessment):
    """Generate simple effectiveness analysis based on purpose"""
    purpose = requirements.get('PROMPT_PURPOSE', '').lower()
    analysis = []
Ôªø
    # Basic purpose alignment check
    purpose_words = purpose.split()
    alignment_score = sum(1 for word in purpose_words if word in prompt.lower())
Ôªø
    if alignment_score > len(purpose_words) * 0.5:
        analysis.append("‚úÖ Good alignment with stated purpose")
    else:
        analysis.append("‚ö†Ô∏è Could improve alignment with stated purpose")
Ôªø
    # Task-specific checks
    if 'analysis' in purpose or 'analyze' in purpose:
        if any(word in prompt.lower() for word in ['step', 'method', 'systematic']):
            analysis.append("‚úÖ Includes structured analytical approach")
        else:
            analysis.append("‚ö†Ô∏è Would benefit from structured analytical steps")
Ôªø
    elif 'create' in purpose or 'generate' in purpose:
        if 'example' in prompt.lower():
            analysis.append("‚úÖ Provides guidance through examples")
        else:
            analysis.append("‚ö†Ô∏è Could benefit from examples or templates")
Ôªø
    elif 'solve' in purpose or 'problem' in purpose:
        if any(word in prompt.lower() for word in ['step', 'approach', 'method']):
            analysis.append("‚úÖ Provides problem-solving methodology")
        else:
            analysis.append("‚ö†Ô∏è Needs clearer problem-solving steps")
Ôªø
    return "**Purpose Effectiveness Analysis:**\n" + '\n'.join(f"  {item}" for item in analysis)
Ôªø
---
Ôªø
## GENERATION STRATEGIES
Ôªø
### Component Building Blocks
Ôªø
| Component Type | Purpose | Quality Impact |
|----------------|---------|----------------|
| **Main Instruction** | Core task definition | +15-20 points |
| **Role Definition** | Establish expertise/persona | +10-15 points |
| **Input Specification** | Define expected inputs | +10 points |
| **Output Specification** | Define expected outputs | +10 points |
| **Step-by-Step Process** | Break down complex tasks | +10-15 points |
| **Examples** | Provide concrete illustrations | +10-15 points |
| **Constraints** | Set boundaries and limits | +5-10 points |
| **Error Handling** | Manage edge cases | +10 points |
| **Validation Checks** | Ensure quality outputs | +5-10 points |
| **Reasoning Framework** | Add thinking structure | +15-20 points |
Ôªø
### Advanced Techniques Integration
Ôªø
#### When to Add Each Technique:
- **Loop 1-2**: Basic structure and instructions
- **Loop 3**: Chain-of-Thought or reasoning steps
- **Loop 4**: Examples and validation
- **Loop 5**: Error handling and edge cases
- **Loop 6**: Final optimizations and polish
Ôªø
### Architecture Patterns
Ôªø
#### Simple Task Pattern:
```
1. Clear instruction
2. Input format
3. Output format
4. Basic constraints
```
Ôªø
#### Complex Analysis Pattern:
```
1. Role and expertise
2. Context and background
3. Step-by-step methodology
4. Reasoning framework
5. Validation criteria
6. Output structure
```
Ôªø
#### System Framework Pattern:
```
1. System architecture
2. Component definitions
3. Integration points
4. Control flow
5. Error recovery
6. Meta-instructions
```
Ôªø
---
Ôªø
## EXAMPLE GENERATION SESSION
Ôªø
```markdown
# Prompt Generation Session - 2024-01-14 10:00:00
## Requirements Specification:
"I need a prompt that helps analyze customer feedback and extract actionable insights"
Ôªø
---
Ôªø
## Loop 1 - 2024-01-14 10:00:30
**Build Stage:** foundation
**Goal:** Build foundation layer
**Architecture:** Analytical/Reasoning
Ôªø
### OBSERVE - Current State
**Starting Point:** Requirements only
**Components Needed:** context method reasoning validation
Ôªø
### ORIENT - Build Strategy
**Strategy:** Establish basic prompt skeleton
**Build Focus:** Core Structure
Ôªø
### DECIDE - Implementation Plan
**Primary Component:** main_instruction
Ôªø
### ACT - Building Prompt
**Current Prompt:**
```
# Customer Feedback Analysis
Ôªø
## Instructions
You are tasked with analyzing customer feedback to extract actionable insights.
Your goal is to identify patterns, sentiment, and specific improvement suggestions.
Ôªø
## Input
[Customer feedback text]
Ôªø
## Output
- Overall sentiment
- Key themes
- Actionable recommendations
```
Ôªø
### CHECK - Quality Assessment
**New Score:** 35/100
Ôªø
---
Ôªø
## Loop 2 - 2024-01-14 10:02:00
[Adds reasoning framework, validation steps...]
Ôªø
---
Ôªø
## FINAL GENERATED PROMPT
**Quality Score:** 92/100
[Complete generated prompt with all components...]
```
Ôªø
---
Ôªø
## BUILD LEVEL PROGRESSION
Ôªø
### Foundation (Loops 1-2)
- Core instructions
- Basic I/O specifications
- Initial constraints
Ôªø
### Enhancement (Loops 3-4)
- Reasoning chains
- Validation methods
- Examples
Ôªø
### Polish (Loops 5-6)
- Edge case handling
- Performance optimization
- Final refinements
Ôªø
---
Ôªø
**Implementation Notes for Claude Code:**
- Use Write tool to create initial prompt_gen.md
- Use Edit tool to update sections iteratively  
- Use Write tool to create final clean prompt file
- Replace bash variables with Python-style variable tracking
- Use TodoWrite tool to track loop progress
- Maintain all core OODA loop logic and quality scoring
- Preserve complete component library and architecture patterns
Ôªø
**Remember:** Each loop builds upon the previous, gradually constructing a comprehensive, high-quality prompt tailored to the specific requirements.
